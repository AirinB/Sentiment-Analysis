{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educated-delaware",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "prompt-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-norway",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "knowing-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./archive/Train.csv')\n",
    "test_data = pd.read_csv('./archive/Test.csv')\n",
    "validation_data = pd.read_csv('./archive/Valid.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "resident-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = train_data['text'].values, train_data['label'].values\n",
    "x_val, y_val = validation_data['text'].values, validation_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "round-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "word_to_ix = vectorizer.fit(train_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "scenic-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "tr_data_vecs = torch.FloatTensor(word_to_ix.transform(x_tr).toarray())\n",
    "tr_labels = y_tr.tolist()\n",
    "\n",
    "val_data_vecs = torch.FloatTensor(word_to_ix.transform(x_val).toarray())\n",
    "val_labels = y_val.tolist()\n",
    "\n",
    "tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "val_data_loader = [(sample, label) for sample, label in zip(val_data_vecs, val_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "moving-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iterator = DataLoader(tr_data_loader,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            )\n",
    "\n",
    "valid_iterator = DataLoader(val_data_loader,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "asian-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "involved-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "      \n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "       \n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "electrical-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = VOCAB_SIZE\n",
    "\n",
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "impaired-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "legitimate-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dramatic-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def class_accuracy(preds, y):\n",
    "   \n",
    "    rounded_preds = preds.argmax(1)\n",
    "    \n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    " \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "gentle-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for texts, labels in iterator:\n",
    "     \n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "    \n",
    "        predictions = model(texts)\n",
    "\n",
    "       \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = class_accuracy(predictions, labels)\n",
    "        \n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "      \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "affecting-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The evaluation is done on the validation dataset\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # On the validation dataset we don't want training so we need to set the model on evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Also tell Pytorch to not propagate any error backwards in the model\n",
    "    # This is needed when you only want to make predictions and use your model in inference mode!\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n",
    "        for texts, labels in iterator:\n",
    "            # We copy the text and label to the correct device\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "           \n",
    "            \n",
    "            acc = class_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "confused-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This is just for measuring training time!\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "common-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.89%\n",
      "\t Val. Loss: 1.357 |  Val. Acc: 84.93%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.89%\n",
      "\t Val. Loss: 1.369 |  Val. Acc: 84.95%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.90%\n",
      "\t Val. Loss: 1.367 |  Val. Acc: 84.95%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.89%\n",
      "\t Val. Loss: 1.374 |  Val. Acc: 84.83%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.89%\n",
      "\t Val. Loss: 1.379 |  Val. Acc: 84.91%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.89%\n",
      "\t Val. Loss: 1.382 |  Val. Acc: 84.97%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.90%\n",
      "\t Val. Loss: 1.386 |  Val. Acc: 84.85%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.90%\n",
      "\t Val. Loss: 1.393 |  Val. Acc: 84.89%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 84.87%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.402 |  Val. Acc: 84.85%\n",
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.90%\n",
      "\t Val. Loss: 1.408 |  Val. Acc: 84.93%\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.415 |  Val. Acc: 84.95%\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.421 |  Val. Acc: 84.81%\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.435 |  Val. Acc: 84.79%\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.428 |  Val. Acc: 84.83%\n",
      "Epoch: 16 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.437 |  Val. Acc: 84.73%\n",
      "Epoch: 17 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.446 |  Val. Acc: 84.73%\n",
      "Epoch: 18 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.91%\n",
      "\t Val. Loss: 1.445 |  Val. Acc: 84.81%\n",
      "Epoch: 19 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.93%\n",
      "\t Val. Loss: 1.449 |  Val. Acc: 84.79%\n",
      "Epoch: 20 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.458 |  Val. Acc: 84.89%\n",
      "Epoch: 21 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.459 |  Val. Acc: 84.77%\n",
      "Epoch: 22 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.468 |  Val. Acc: 84.87%\n",
      "Epoch: 23 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.470 |  Val. Acc: 84.83%\n",
      "Epoch: 24 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.476 |  Val. Acc: 84.93%\n",
      "Epoch: 25 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.92%\n",
      "\t Val. Loss: 1.480 |  Val. Acc: 84.75%\n",
      "Epoch: 26 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.487 |  Val. Acc: 84.91%\n",
      "Epoch: 27 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.491 |  Val. Acc: 84.89%\n",
      "Epoch: 28 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.93%\n",
      "\t Val. Loss: 1.499 |  Val. Acc: 84.91%\n",
      "Epoch: 29 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.498 |  Val. Acc: 84.85%\n",
      "Epoch: 30 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.505 |  Val. Acc: 84.77%\n",
      "Epoch: 31 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.518 |  Val. Acc: 84.83%\n",
      "Epoch: 32 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.517 |  Val. Acc: 84.75%\n",
      "Epoch: 33 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.523 |  Val. Acc: 84.79%\n",
      "Epoch: 34 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.529 |  Val. Acc: 84.89%\n",
      "Epoch: 35 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.529 |  Val. Acc: 84.71%\n",
      "Epoch: 36 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.541 |  Val. Acc: 84.85%\n",
      "Epoch: 37 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.545 |  Val. Acc: 84.89%\n",
      "Epoch: 38 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.548 |  Val. Acc: 84.87%\n",
      "Epoch: 39 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.555 |  Val. Acc: 84.75%\n",
      "Epoch: 40 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.554 |  Val. Acc: 84.69%\n",
      "Epoch: 41 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.94%\n",
      "\t Val. Loss: 1.564 |  Val. Acc: 84.71%\n",
      "Epoch: 42 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.566 |  Val. Acc: 84.85%\n",
      "Epoch: 43 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.570 |  Val. Acc: 84.75%\n",
      "Epoch: 44 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.575 |  Val. Acc: 84.77%\n",
      "Epoch: 45 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 84.65%\n",
      "Epoch: 46 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.96%\n",
      "\t Val. Loss: 1.587 |  Val. Acc: 84.75%\n",
      "Epoch: 47 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.595 |  Val. Acc: 84.81%\n",
      "Epoch: 48 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.598 |  Val. Acc: 84.73%\n",
      "Epoch: 49 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.606 |  Val. Acc: 84.87%\n",
      "Epoch: 50 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
      "\t Val. Loss: 1.605 |  Val. Acc: 84.77%\n"
     ]
    }
   ],
   "source": [
    "# Define the epoch number parameter\n",
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# We loop forward on the epoch number\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model on the training set using the dataloader\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    metrics['train_loss'].append(train_loss)\n",
    "    metrics['train_accuracy'].append(train_acc)\n",
    "    # And validate your model on the validation set\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    metrics['dev_loss'].append(valid_loss)\n",
    "    metrics['dev_accuracy'].append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # If we find a better model, we save the weights so later we may want to reload it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "innovative-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "level-medication",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-c9134e14f5e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dev_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dev loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[1;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_LinePlotter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_semantics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m     p = _LinePlotter(\n\u001b[0m\u001b[0;32m    686\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\relational.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, variables, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend)\u001b[0m\n\u001b[0;32m    365\u001b[0m         )\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\_core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_semantic_mappings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\_core.py\u001b[0m in \u001b[0;36massign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"wide\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m             plot_data, variables = self._assign_variables_wideform(\n\u001b[0m\u001b[0;32m    663\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\seaborn\\_core.py\u001b[0m in \u001b[0;36m_assign_variables_wideform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m                     \u001b[1;31m# TODO is there a safer/more generic way to ensure Series?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m                     \u001b[1;31m# sort of like np.asarray, but for pandas?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                     \u001b[0mdata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    570\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of a 0-d tensor"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "sns.lineplot(data=metrics['train_loss'], ax=ax[0], label='train loss')\n",
    "sns.lineplot(data=metrics['dev_loss'], ax=ax[0], label='dev loss')\n",
    "\n",
    "sns.lineplot(data=metrics['train_accuracy'], ax=ax[1], label='train acc')\n",
    "sns.lineplot(data=metrics['dev_accuracy'], ax=ax[1], label='dev acc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
